{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Replace this with your actual document or whatever (Web page, Word...)\n",
    "loader = PyPDFLoader(\"loi-n-01-00-portant-organisation-de-lenseignement-supérieur.pdf\")\n",
    "data = loader.load()\n",
    "print(len(data))\n",
    "print(type(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=90,\n",
    ")\n",
    "doc=text_splitter.split_documents(data)\n",
    "\n",
    "print(len(doc))\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.vectorstores.faiss.FAISS'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = FAISS.from_documents(documents=doc, embedding=embeddings)\n",
    "print(type(vector_store))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store = FAISS.load_local(r\"C:\\Users\\LENOVO\\Desktop\\DATA_SCIENCE\\Deep_Learning\\ReGenuis--RAG\\faiss_index\",embeddings,allow_dangerous_deserialization=True)\n",
    "\n",
    "# retriever = vector_store.as_retriever()\n",
    "# print(type(retriever))\n",
    "# vector_store.similarity_search(query,3)\n",
    "retriever = vector_store.as_retriever()\n",
    "# retrieved_docs = retriever.invoke(query)\n",
    "# print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------QUERY--------------\n",
    "query = \"l'artcile 21\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "prompt = f\"Contexte:\\n{context}\\n\\nQuestion: {query}\\rReponse:\"\n",
    "#----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_18664\\2743731484.py:4: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = llm(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Selon l'article 18 du texte, le budget de l'université comprend les recettes suivantes :\n",
      "\n",
      "1. Les subventions de l'État (9)\n",
      "2. Les droits perçus au titre de la formation continue (9)\n",
      "3. Les revenus, recettes et tous autres produits autorisés par la législation et la réglementation en vigueur (9)\n",
      "4. Les produits et bénéfices provenant des travaux de recherche et des prestations de services (9)\n",
      "5. Les produits et bénéfices provenant de ses opérations et de son patrimoine (9)\n",
      "6. Les produits d'emprunts contractés auprès d'organismes financiers internationaux (9)\n",
      "7. Les ressources à caractère occasionnel générées par la vente de biens ou valeurs, ou de toute autre origine (9)\n",
      "8. Les avances remboursables du Trésor (6)\n",
      "\n",
      "En outre, l'article 18 indique que le président de l'université peut faire appel, à titre consultatif, à toute personne qualifiée pour donner son avis sur certaines questions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm=OllamaLLM(model='llama3.2:latest')\n",
    "res = llm(prompt)\n",
    "print(\"Answer:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous n'avez pas fourni l'article 21. Je ne peux donc pas y répondre.  Veuillez fournir le texte de l'article 21 pour que je puisse vous aider.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import os\n",
    "# os.environ['GOOGLE_API_KEY'] = 'AIzaSyDPY7XqjkX_G-hiS-xaQ0oMuEuAIDXNywU'\n",
    "api_key = 'AIzaSyDPY7XqjkX_G-hiS-xaQ0oMuEuAIDXNywU'\n",
    "\n",
    "llm2 = GoogleGenerativeAI(model=\"models/gemini-1.5-pro-latest\",google_api_key=api_key)\n",
    "\n",
    "# Wrap the prompt in a HumanMessage\n",
    "# res = llm2([HumanMessage(content=prompt)])\n",
    "# Print the model's response\n",
    "# print(\"Answer:\", res)\n",
    "\n",
    "res = llm2.invoke(prompt)\n",
    "print(res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object list_models at 0x00000265BDB547C0>\n",
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/text-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 (Legacy)',\n",
      "      description='A legacy model that understands text and generates text as an output',\n",
      "      input_token_limit=8196,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
      "      temperature=0.7,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Latest',\n",
      "      description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
      "                   'February 15th, 2025. Move to a newer Gemini version.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
      "      description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
      "                   'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
      "                   'version.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-vision-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
      "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
      "                   'Move to a newer Gemini version.'),\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
      "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
      "                   'Move to a newer Gemini version.'),\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-1.5-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
      "                   'million tokens.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro 001',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Pro 002',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro-exp-0801',\n",
      "      base_model_id='',\n",
      "      version='exp-0801',\n",
      "      display_name='Gemini 1.5 Pro Experimental 0801',\n",
      "      description=('Experimental release (August 1st, 2024) of Gemini 1.5 Pro, our mid-size '\n",
      "                   'multimodal model that supports up to 2 million tokens, with across the board '\n",
      "                   'improvements. Replaced by Gemini-1.5-pro-002 (stable).'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='exp-1206',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
      "                   'across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 001',\n",
      "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in May of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-001-tuning',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
      "      description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
      "                   'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
      "      input_token_limit=16384,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
      "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='exp-1206',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Flash 002',\n",
      "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B 001',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
      "                   'released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
      "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
      "                   'smallest and most cost effective Flash model. Replaced by '\n",
      "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
      "      description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
      "                   'smallest and most cost effective Flash model. Replaced by '\n",
      "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/learnlm-1.5-pro-experimental',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='LearnLM 1.5 Pro Experimental',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
      "                   'mid-size multimodal model that supports up to 2 million tokens.'),\n",
      "      input_token_limit=32767,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-exp-1114',\n",
      "      base_model_id='',\n",
      "      version='exp-1121',\n",
      "      display_name='Gemini Experimental 1121',\n",
      "      description='Experimental release (November 21st, 2024) of Gemini.',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-exp-1121',\n",
      "      base_model_id='',\n",
      "      version='exp-1121',\n",
      "      display_name='Gemini Experimental 1121',\n",
      "      description='Experimental release (November 21st, 2024) of Gemini.',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-exp-1206',\n",
      "      base_model_id='',\n",
      "      version='exp_1206',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/text-embedding-004',\n",
      "      base_model_id='',\n",
      "      version='004',\n",
      "      display_name='Text Embedding 004',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "from google.generativeai import models\n",
    "\n",
    "available_models = models.list_models()\n",
    "print(available_models)\n",
    "for model in models.list_models():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- WE HAVE TO PAY TO USE OPENAI --------------------\n",
    "\n",
    "# os.environ['OPENAI_API_KEY']='Your_openai_ApiKey'\n",
    "# from langchain_openai import OpenAI\n",
    "# llm3 = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT THE PROMPT LOOKS LIKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "universitaires concernés. \n",
      "Les universités sont habilitées à servir, à partir de leurs recettes propres provenant des travaux de recherche et des prestations \n",
      "de services, des indemnités complémentaires à leurs personnels à titre d'encouragement et d'émulation. \n",
      " \n",
      "Article 18 \n",
      "Le budget de l'université comprend : \n",
      "En recettes: \n",
      "9 les subventions de l'Etat ; \n",
      "9 les droits perçus au titre de la formation continue ; \n",
      "9 les revenus, recettes et tous autres produits autorisés par la législation et la réglementation en vigueur; \n",
      "9 les produits et bénéfices provenant des travaux de recherch es et des prestations de services, notamment des travaux \n",
      "d'expertises ; \n",
      "9 les produits et bénéfices provenant de ses opérations et de son patrimoine ; \n",
      "9 les produits d'emprunts contractés auprès d'organismes financiers internationaux ; \n",
      "9 les ressources à caractère occasionnel générées par la vente de biens ou valeurs, ou de toute autre origine ; \n",
      "9 les avances remboursables du Trésor;\n",
      "6 \n",
      " \n",
      "9 prend toutes mesures visant à améliorer la gestion de l'université ; \n",
      "9 propose toutes réformes des formations assurées au sein de  l'université et prend toutes mesures de nature pédagogique \n",
      "visant à améliorer la qualité de la formation ; \n",
      "9 établit son règlement intérieur et celui de l'université et  les soumet à l'autorité gouvernementale de tutelle pour \n",
      "approbation dans un délai maximum de trente jours ; passé ce délai, le règlement est réputé approuvé; \n",
      "9 donne son avis sur les demandes d'accréditation présentées par les établissements universitaires ; \n",
      "9 approuve les projets de création de filières de formation et de recherche ; \n",
      "9 adopte le projet de budget de l'université ; \n",
      "9 répartit les crédits entre les différents établissements universit aires, les services d'université et les services communs de \n",
      "l'université ; \n",
      "9 fixe les régimes des indemnités complémentaires visées au 3e alinéa de l'article 17 ci-dessous ;\n",
      "siège de l'université ; \n",
      "le ou les directeurs des académies régionales d'éducation et de formation (AREF) concernées ; \n",
      "2. sept représentants des secteurs économiques et socia ux dont les présidents des chambres professionnelles et un \n",
      "représentant de l'enseignement supérieur privé ; \n",
      "3. trois représentants élus par et parmi les enseignants-chercheurs de chaque ét ablissement universitaire en respectant la \n",
      "représentativité des différentes catégories de corps enseignants ; \n",
      "4. trois représentants élus par et parmi les personnels administratifs et techniques de l'université ; \n",
      "5. trois représentants élus par et parmi les étudiants de l'université ; \n",
      "6. les chefs d'établissements universitaires de l'université concernée ; \n",
      "7. un chef d'établissement d'enseignement supérieur public ne relevant pas de l'université, désigné par le conseil de \n",
      "coordination prévu à l'article 28 ci-dessous. \n",
      "Le président peut faire appel, à titre consultatif, à toute personne qualifiée.\n",
      "4 \n",
      "Les universités ont vocation normale à dispenser tous enseigneme nts et formations initiales et  à préparer et délivrer les \n",
      "diplômes y afférents. \n",
      "Elles organisent des formations continues au profit des pe rsonnes engagées ou non dans la vie active pour répondre à des \n",
      "besoins individuels ou collectifs. \n",
      " \n",
      "Article 4 \n",
      "Les universités sont créées par la loi conformément à l'article 46 de la Constitution. Elles sont des établissements publics \n",
      "dotés de la personnalité morale et de l'autonomie administrative et financière. \n",
      "Elles sont placées sous la tutelle de l'Etat, laquelle a pour obj et de faire respecter par les organes compétents des universit és, \n",
      "les dispositions de la présente loi, en particulier celles rela tives aux missions qui leur sont dévolues et, de manière général e, \n",
      "de veiller en ce qui les concerne, à l'application de la législ ation et de la réglementation concernant les établissements \n",
      "publics. \n",
      " \n",
      "Article 5\n",
      "\n",
      "Question: qu'est ce que comprend Le budget de l'université  \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
